{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPk8WQ78Bnv5IsHfOgry9i3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xeesto/UEP/blob/dev/BigData_notatki_zaliczenie_MATI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wczytanie Sparka"
      ],
      "metadata": {
        "id": "Z0iNY63yasHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTeC0PP9aai-"
      },
      "outputs": [],
      "source": [
        "# Ustaw wersjƒô jako parametr\n",
        "SPARK_VERSION=\"3.5.6\"\n",
        "\n",
        "# Instalacja OpenJDK 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Pobranie Apache Spark z okre≈õlonƒÖ wersjƒÖ\n",
        "!wget -q http://www.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# Rozpakowanie archiwum Spark\n",
        "!tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# Instalacja findspark i pyspark\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install -q pyspark==$SPARK_VERSION\n",
        "\n",
        "# Ustalamy zmienne ≈õrodowiskowe.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{SPARK_VERSION}-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "findspark.init(f\"spark-{SPARK_VERSION}-bin-hadoop3\")\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "spark = SparkSession.builder.appName('abc').getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "phEL25ZHayl4",
        "outputId": "a79ce55c-7370-4908-accd-a99759f0ae67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'findspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2832af52c1b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL | podstawowa obr√≥bka danych | pierwsze kroki w dataframie"
      ],
      "metadata": {
        "id": "cswgS33ka2C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('all_weekly_excess_deaths.csv', header=True, inferSchema=True, sep=';')"
      ],
      "metadata": {
        "id": "UgmUtx-ibDy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "xRrf3CvHb6H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tworzenie w≈Çasnej schema"
      ],
      "metadata": {
        "id": "ZFSLOlDNcXph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"CUST_ID\", IntegerType(), True),\n",
        "    StructField(\"BALANCE\", DoubleType(), True),\n",
        "    StructField(\"BALANCE_FREQUENCY\", DoubleType(), True),\n",
        "    StructField(\"PURCHASES\", DoubleType(), True),\n",
        "    StructField(\"ONEOFF_PURCHASES\", DoubleType(), True),\n",
        "    StructField(\"INSTALLMENTS_PURCHASES\", DoubleType(), True),\n",
        "    StructField(\"CASH_ADVANCE\", DoubleType(), True),\n",
        "    StructField(\"PURCHASES_FREQUENCY\", DoubleType(), True),\n",
        "    StructField(\"ONEOFF_PURCHASES_FREQUENCY\", DoubleType(), True),\n",
        "    StructField(\"PURCHASES_INSTALLMENTS_FREQUENCY\", DoubleType(), True),\n",
        "    StructField(\"CASH_ADVANCE_FREQUENCY\", DoubleType(), True),\n",
        "    StructField(\"CASH_ADVANCE_TRX\", IntegerType(), True),\n",
        "    StructField(\"PURCHASES_TRX\", IntegerType(), True),\n",
        "    StructField(\"CREDIT_LIMIT\", IntegerType(), True),\n",
        "    StructField(\"PAYMENTS\", DoubleType(), True),\n",
        "    StructField(\"MINIMUM_PAYMENTS\", DoubleType(), True),\n",
        "    StructField(\"PRC_FULL_PAYMENT\", DoubleType(), True),\n",
        "    StructField(\"TENURE\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Load the data with the defined schema\n",
        "cc_gen = spark.read.csv(\"CC_GENERAL.csv\", header=True, schema=schema)"
      ],
      "metadata": {
        "id": "iryF2VbCb8VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podzielenie tekstu tak aby otrzymaƒá nowa kolumnƒô do spacji"
      ],
      "metadata": {
        "id": "7r0x20kJeWUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zmiana typu kolumny ze STRINGA na INT\n",
        "df = df.withColumn(\"total_deaths_number\", col(\"total_deaths_number\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "SbRt1TeZfWE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Struktura i podglƒÖd danych\n",
        "\n",
        "| Komenda                | Opis                                         |\n",
        "| ---------------------- | -------------------------------------------- |\n",
        "| `df.show(n)`           | Wy≈õwietla pierwsze `n` wierszy               |\n",
        "| `df.printSchema()`     | Pokazuje strukturƒô DataFrame                 |\n",
        "| `df.columns`           | Zwraca listƒô nazw kolumn                     |\n",
        "| `df.dtypes`            | Zwraca listƒô typ√≥w danych                    |\n",
        "| `df.describe().show()` | Statystyki opisowe (≈õrednia, min, max itd.)  |\n",
        "| `df.head(n)`           | Zwraca pierwsze `n` wierszy jako obiekty Row |\n",
        "| `df.limit(n)`          | Zwraca nowy DataFrame z `n` wierszami        |\n"
      ],
      "metadata": {
        "id": "kt1hy5IafW3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÑ ≈ÅƒÖczenie i transformacje\n",
        "\n",
        "| Komenda                      | Opis                   |\n",
        "| ---------------------------- | ---------------------- |\n",
        "| `df.groupBy(\"col\").agg(...)` | Grupowanie i agregacje |\n",
        "| `df.groupBy(\"col\").count()`  | Liczy wystƒÖpienia      |\n",
        "| `df.agg({...})`              | Agregacje globalne     |\n",
        "| `df.orderBy(\"col\")`          | Sortowanie             |\n"
      ],
      "metadata": {
        "id": "5h0Hx7BgfkgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÑ ≈ÅƒÖczenie i transformacje\n",
        "\n",
        "| Komenda                      | Opis                   |\n",
        "| ---------------------------- | ---------------------- |\n",
        "| `df.groupBy(\"col\").agg(...)` | Grupowanie i agregacje |\n",
        "| `df.groupBy(\"col\").count()`  | Liczy wystƒÖpienia      |\n",
        "| `df.agg({...})`              | Agregacje globalne     |\n",
        "| `df.orderBy(\"col\")`          | Sortowanie             |\n"
      ],
      "metadata": {
        "id": "LqnmXrupfrj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé Filtrowanie i wybieranie\n",
        "\n",
        "| Komenda                               | Opis                                         |\n",
        "| ------------------------------------- | -------------------------------------------- |\n",
        "| `df.select(\"col1\", \"col2\")`           | Wybiera kolumny                              |\n",
        "| `df.filter(condition)`                | Filtrowanie wierszy (alias: `df.where(...)`) |\n",
        "| `df.drop(\"col\")`                      | Usuwa kolumnƒô                                |\n",
        "| `df.withColumn(\"new\", ...)`           | Dodaje lub nadpisuje kolumnƒô                 |\n",
        "| `df.withColumnRenamed(\"old\", \"new\")`  | Zmienia nazwƒô kolumny                        |\n",
        "| `df.distinct()`                       | Usuwa duplikaty                              |\n",
        "| `df.dropDuplicates([\"col1\", \"col2\"])` | Duplikaty na podstawie kolumn                |\n"
      ],
      "metadata": {
        "id": "YCcUOvQwfdRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nowa kolumna bazujƒÖca na istniejƒÖcej kolumnie z pierwszymi 4 znakami\n",
        "df = df.withColumn(\"total_deaths_prefix\", substring(col(\"total_deaths\"), 1, 4))"
      ],
      "metadata": {
        "id": "6x7NcYuckLhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podziel tekst w kolumnie total_deaths na dwie czƒô≈õci: przed i po pierwszej spacji\n",
        "split_col = split(col(\"total_deaths\"), \" \", 2)\n",
        "\n",
        "# Nowa kolumna o nazwie \"total_deaths_number\" idzie na sam koniec danych i jest rozdzielona po spacji bazujƒÖc na kolumnie \"total_deaths\"\n",
        "df = df.withColumn(\"total_deaths_number\", split_col.getItem(0))"
      ],
      "metadata": {
        "id": "fikRupgzkUUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybranie kolumn, agregacja po kolumnie groupBy\n",
        "df_total = (\n",
        "       df.select('total_deaths_number', 'country', 'region')\n",
        "      .groupBy('region')\n",
        "      .agg(max('total_deaths_number').alias('last_week_deaths'))\n",
        "      .orderBy('last_week_deaths', ascending=False)\n",
        ")\n",
        "\n",
        "df_total.show()"
      ],
      "metadata": {
        "id": "FsUjmQvjj7mD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}